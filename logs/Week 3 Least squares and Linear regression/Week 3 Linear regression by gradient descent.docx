Linear regression by gradient descent

we need to find cost function of theta (J(Î¸)) such that it is high initially with a very small theta (zero or close to zero)
Then, we will update Î¸ in a very small step until there is hardly any change in J(Î¸).
This is the Î¸ we are looking for.

J(Î¸) = (1/2m)(y - XÎ¸)^T(y - XÎ¸)

J(Î¸) pythonic code representation:
cost = (1/(2*m)) * np.sum(error ** 2) # error is a vector, y_pred - y, a difference of two vectors 

Î¸ update rule: Î¸ = Î¸ - alpha.1/m.X^T(XÎ¸ - y)

Suggestion roadmap:

now do Linear Regression using scikit-learn, Then:
Step	Topic	                        Why
1	    Linear Regression	            Youâ€™ve mastered it ðŸŽ¯
2	    Multiple Linear Regression	    Add more features
3	    Polynomial Regression	        Model non-linear data
4	    Regularization (Ridge, Lasso)	Prevent overfitting
5	    Logistic Regression	For classification problems